{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SafeDriver.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6QIg2AV7ke_9"},"source":["# Step 1: Import packages\n"]},{"cell_type":"code","metadata":{"id":"r2lY0HMhkMKs"},"source":["!pip install --upgrade tensorflow"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rSgqFyptkb8r"},"source":["import os\n","import glob\n","import numpy as np\n","from tensorflow import keras \n","from tensorflow.keras import layers\n","import tensorflow as tf\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout, Conv2D, MaxPool2D, Flatten\n","from numpy import array\n","from numpy import hstack\n","from sklearn import preprocessing\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.utils import shuffle\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import random\n","from random import sample\n","import tempfile\n","import math\n","print(tf.__version__) \n","tf.test.gpu_device_name()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C-f9NDDRko2e"},"source":["# Step 2: Mount Google drive"]},{"cell_type":"code","metadata":{"id":"t3Ct4ci0kqGC"},"source":["import os, sys\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/My Drive/Colab Notebooks')\n","nb_path = '/content/notebooks'\n","os.symlink('/content/gdrive/My Drive/Colab Notebooks', nb_path)\n","sys.path.insert(0, nb_path)\n","os.listdir()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zqrctpz0HHlw"},"source":["# Step 3: Data preprocessing"]},{"cell_type":"markdown","metadata":{"id":"h9lAflRRH9Gn"},"source":["## a. Basic data features"]},{"cell_type":"code","metadata":{"id":"7kKv9LiLmzwy"},"source":["n_features = 6 # ax, ay, az, jx, jy, jz\n","n_class = 6    # idle, cruise, start, stop, left, right\n","n_steps = 30   # customize"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oDEmlwB9m0QA"},"source":["## b. Read CSV files from drive"]},{"cell_type":"code","metadata":{"id":"uenG0lagni9w"},"source":["train_path = '/content/drive/My Drive/Colab Notebooks/Training_new/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VVzrxrGqmwFd"},"source":["def read_file(file_path, file_name, n_class):\n","  '''\n","  This function reads a CSV file, generate a dataset that contains 3-axis acceleration and jerk,\n","  and classification of motions in one-hot format.\n","  '''\n","  # Load CSV file\n","  print(file_name)\n","  read_data = pd.read_csv(file_path + file_name)  \n","  # read all features in Dataframe\n","  ax = read_data['ax'].to_numpy()\n","  ay = read_data['ay'].to_numpy()\n","  az = read_data['az'].to_numpy()\n","\n","  jx = read_data['jx'].to_numpy()\n","  jy = read_data['jy'].to_numpy()\n","  jz = read_data['jz'].to_numpy()\n","  m_int = read_data['class'].to_numpy()\n","\n","  acc = hstack((ax.reshape(-1, 1), ay.reshape(-1, 1), az.reshape(-1, 1)))\n","  jerk = hstack((jx.reshape(-1, 1), jy.reshape(-1, 1), jz.reshape(-1, 1)))\n","\n","  # Convert motion classification to one-hot format\n","  motion = []\n","  for i in range(m_int.shape[0]):\n","    m_onehot = np.eye(n_class)[m_int[i]]\n","    motion.append(m_onehot)\n","\n","  motion = array(motion)\n","\n","  dataset = hstack((acc, jerk, motion))\n","\n","  return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5VCY95WPtTm5"},"source":["### Plot tool"]},{"cell_type":"code","metadata":{"id":"3sLivvCXNEsQ"},"source":["def plot_data(dataset):\n","  '''\n","  This is a plot tool to display the informatoin of dataset,\n","  including acceleration, jerk and classification\n","  '''\n","  plt.figure(figsize=(50, 12.5))\n","  cc = ['k', 'b', 'g', 'r', 'm', 'y']\n","\n","  plt.subplot(311)  \n","  plt.plot(dataset[:,0], label = 'ax')\n","  plt.plot(dataset[:,1], label = 'ay')\n","  plt.plot(dataset[:,2], label = 'az')\n","  marker = 0\n","  c = np.argmax(dataset[0,6:])\n","  for x in range(0, dataset.shape[0]):\n","      if (np.argmax(dataset[x,6:]) != c):\n","          plt.axvspan(marker, x, facecolor=cc[c], alpha=0.2)\n","          marker = x\n","          c = np.argmax(dataset[x,6:])\n","  plt.axvspan(marker, dataset.shape[0], facecolor=cc[c], alpha=0.2)\n","  plt.grid()\n","  plt.title('acc')\n","  plt.legend()\n","\n","  plt.subplot(312)\n","  plt.plot(dataset[:,3], label = 'jx')\n","  plt.plot(dataset[:,4], label = 'jy')\n","  plt.plot(dataset[:,5], label = 'jz')\n","  marker = 0\n","  c = np.argmax(dataset[0,6:])\n","  for x in range(0, dataset.shape[0]):\n","      if (np.argmax(dataset[x,6:]) != c):\n","          plt.axvspan(marker, x, facecolor=cc[c], alpha=0.2)\n","          marker = x\n","          c = np.argmax(dataset[x,6:])\n","  plt.axvspan(marker, dataset.shape[0], facecolor=cc[c], alpha=0.2)\n","  plt.grid()\n","  plt.title('jerk')\n","  plt.legend()\n","\n","  plt.subplot(313)\n","  classes = [np.argmax(dataset[i][6:]) for i in range(len(dataset))]\n","  plt.plot(classes)\n","  plt.ylim([0, 5])\n","  plt.grid()\n","  plt.title('class')\n","\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zWOGzr17dcN0"},"source":["## c. Separate training  data and valid data"]},{"cell_type":"code","metadata":{"id":"9XzASJ3BdTUS"},"source":["def split_files(file_path, ratio):\n","  '''\n","  This function separates our files into training files and validation files.\n","  '''\n","  # ratio: (# of valid file) / (# of all files)\n","\n","  all_files = os.listdir(file_path)\n","  valid_num = int(ratio*len(all_files))\n","  valid_files = random.sample(all_files, valid_num)\n","  train_files = list(set(all_files).difference(set(valid_files)))\n","  \n","  return train_files, valid_files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xJRlWY-RWM6D"},"source":["def generate_dataset(file_path, n_class, ratio):\n","  '''\n","  This function read all files in path(file_path), and split the files into training set and validation set,\n","  reading the files in each set, and generate datasets for training and validating.\n","  The output contains two list, each list contains datasets from each files.\n","  '''\n","\n","  # ratio = (# of files for validation)/(# of all files)\n","\n","  train_files, valid_files = split_files(file_path, ratio)\n","  print('train:',train_files)\n","  print('valid:',valid_files)\n","  train_dataset = []\n","  valid_dataset = []\n","\n","  for i in range(len(train_files)):\n","    data_t = read_file(file_path, train_files[i], n_class)\n","    train_dataset.append(data_t)\n","\n","  for i in range(len(valid_files)):\n","    data_v = read_file(file_path, valid_files[i], n_class)\n","    valid_dataset.append(data_v)\n","\n","  return train_dataset, valid_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pa-fZlx1br94"},"source":["################################################################################\n","#       LOAD FILES AND SEPARATING THE DATASET WILL BE USED FOR TRAINING        #\n","################################################################################\n","train_dataset, valid_dataset = generate_dataset(train_path, n_class, 0.3)\n","print(len(train_dataset))\n","print(len(valid_dataset))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iLuDoQY4hvV4"},"source":["## d. Separate diffirent class"]},{"cell_type":"code","metadata":{"id":"NJBfXOIMw5Kz"},"source":["def separate_class(list_of_datasets, n_class):\n","  '''\n","  We have to select a sequential part in dataset that has same class,\n","  then add  the part to the new list that contains parts of dataset of specific class.\n","  '''\n","\n","  # list_of_datasets: a list that contains dataset from each file \n","\n","  datas_0, datas_1,datas_2, datas_3, datas_4, datas_5 = [], [], [], [], [], []\n","  temp = []\n","  last_class = np.argmax(list_of_datasets[0][0][6:])\n","\n","  for j in range(len(list_of_datasets)):\n","    for i in range(list_of_datasets[j].shape[0]):\n","      arr_data = list_of_datasets[j][i]\n","      m_class = np.argmax(arr_data[6:])\n","      if m_class == last_class:\n","        temp.append(arr_data)\n","      else:\n","        if last_class == 0:\n","          datas_0.append(array(temp))\n","        elif last_class == 1:\n","          datas_1.append(array(temp))\n","        elif last_class == 2:\n","          datas_2.append(array(temp))\n","        elif last_class == 3:\n","          datas_3.append(array(temp))\n","        elif last_class == 4:\n","          datas_4.append(array(temp))\n","        elif last_class == 5:\n","          datas_5.append(array(temp)) \n","\n","        temp = []\n","        temp.append(arr_data)\n","\n","      last_class = m_class\n","\n","    if last_class == 0:\n","      datas_0.append(array(temp))\n","    elif last_class == 1:\n","      datas_1.append(array(temp))\n","    elif last_class == 2:\n","      datas_2.append(array(temp))\n","    elif last_class == 3:\n","      datas_3.append(array(temp))\n","    elif last_class == 4:\n","      datas_4.append(array(temp))\n","    elif last_class == 5:\n","      datas_5.append(array(temp))    \n","\n","  return datas_0, datas_1,datas_2, datas_3, datas_4, datas_5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x4A_N8JsYO_z"},"source":["def data_size(data):\n","  '''\n","  A function to calaculate the length of dataset.\n","  '''\n","  size = 0\n","  for i in range(len(data)):\n","    size += data[i].shape[0]\n","  return size, data[0].shape[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fl4n1kHflpE2"},"source":["################################################################################\n","#           SEPARAT THE TRAINING  AND VALIDATION DATASETS BY CLASSES           #\n","################################################################################\n","train_idle, train_cruise, train_acc, train_brake, train_left, train_right = separate_class(train_dataset, n_class)\n","valid_idle, valid_cruise, valid_acc, valid_brake, valid_left, valid_right = separate_class(valid_dataset, n_class)\n","################################################################################\n","#             SHOWING THE SIZE OF TRAINING AND VALIDATION DATASETS             #\n","################################################################################\n","print('RAW DATA:')\n","print('-----TRAINING-----')\n","print('Idle  :', data_size(train_idle))\n","print('Cruise:', data_size(train_cruise))\n","print('Acc   :', data_size(train_acc))\n","print('Brake :', data_size(train_brake))\n","print('Left  :', data_size(train_left))\n","print('Right :', data_size(train_right))\n","print('----VALIDATION----')\n","print('Idle  :', data_size(valid_idle))\n","print('Cruise:', data_size(valid_cruise))\n","print('Acc   :', data_size(valid_acc))\n","print('Brake :', data_size(valid_brake))\n","print('Left  :', data_size(valid_left))\n","print('Right :', data_size(valid_right))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HcM3zl4GGyx7"},"source":["## e. Data augmentation"]},{"cell_type":"code","metadata":{"id":"wMOmMgp6qb7m"},"source":["def rotate_data(dataset, degrees):\n","  '''\n","  This function returns a new dataset which is a result of rotation matrix multiply with input dataset.\n","  '''\n","\n","  # dataset: input dataset \n","  # degrees(theta_x, theta_y, theta_z): input rotation angles for three axis in degree\n","  # Convert  three angles to radian format\n","  theta_x = np.radians(degrees[0])                       \n","  theta_y = np.radians(degrees[1])\n","  theta_z = np.radians(degrees[2])\n","  # Define the rotation matrix\n","  Rx = np.array([[1, 0, 0], [0, math.cos(theta_x), -1*math.sin(theta_x)], [0, math.sin(theta_x), math.cos(theta_x)]])\n","  Ry = np.array([[math.cos(theta_y), 0, math.sin(theta_y)], [0, 1, 0], [-1*math.sin(theta_y), 0, math.cos(theta_y)]])\n","  Rz = np.array([[math.cos(theta_z), -math.sin(theta_z), 0 ], [math.sin(theta_z), math.cos(theta_z), 0], [0, 0, 1]])\n","\n","  ax_r = []\n","  ay_r = []\n","  az_r = []\n","  jx_r = []\n","  jy_r = []\n","  jz_r = []\n","\n","  for i in range(dataset.shape[0]):\n","    # Calculate the dot products\n","    acc = dataset[i][0:3]\n","    acc_ri = acc.dot(Rx).dot(Ry).dot(Rz)\n","    ax_r.append(acc_ri[0])\n","    ay_r.append(acc_ri[1])\n","    az_r.append(acc_ri[2])\n","\n","    jerk = dataset[i][3:6]\n","    jerk_ri = jerk.dot(Rx).dot(Ry).dot(Rz)\n","    jx_r.append(jerk_ri[0])\n","    jy_r.append(jerk_ri[1])\n","    jz_r.append(jerk_ri[2])\n","\n","  ax_r = array(ax_r)\n","  ay_r = array(ay_r)\n","  az_r = array(az_r)\n","  acc_r = hstack((ax_r.reshape(-1, 1), ay_r.reshape(-1, 1), az_r.reshape(-1, 1)))\n","\n","  jx_r = array(jx_r)\n","  jy_r = array(jy_r)\n","  jz_r = array(jz_r)\n","  jerk_r = hstack((jx_r.reshape(-1, 1), jy_r.reshape(-1, 1), jz_r.reshape(-1, 1)))  \n","\n","  motion = []\n","  m_onehot = dataset[:,6:][0]\n","  for i in range(dataset.shape[0]):\n","   motion.append(m_onehot)\n","\n","  dataset_r = hstack((acc_r, jerk_r, array(motion)))\n","\n","  return  dataset_r"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TPrqA72hGvgK"},"source":["def generate_aug_data(list_of_datasets, degrees, aug_ratio):\n","  # list_of_dataset: list of datasets\n","  # degrees: the rotation angles along 3 axis\n","  # aug_ratio: ((# of all datasets in list_of_datasets)/(# chosen datasets))\n","\n","  theta_x, theta_y, theta_z = degrees\n","\n","  if aug_ratio == 0:\n","    return list_of_datasets\n","\n","  pick_num = int(aug_ratio*len(list_of_datasets)/2)\n","  pick_data = sample(list_of_datasets, pick_num)\n","  aug_data = list(list_of_datasets)\n","\n","  batch = int(pick_num/3)\n","\n","  for i in range(batch):\n","    new_data_1 = rotate_data(pick_data[i], (theta_x, 0, 0))\n","    new_data_2 = rotate_data(pick_data[i], (-1*theta_x, 0, 0))\n","    aug_data.append(new_data_1)\n","    aug_data.append(new_data_2)\n","\n","  for i in range(batch, 2*batch):\n","    new_data_3 = rotate_data(pick_data[i], (0, theta_y, 0))\n","    new_data_4 = rotate_data(pick_data[i], (0, -1*theta_y, 0))\n","    aug_data.append(new_data_3)\n","    aug_data.append(new_data_4)\n","\n","  for i in range(2*batch, pick_num):\n","    new_data_5 = rotate_data(pick_data[i], (0, 0, theta_z))\n","    new_data_6 = rotate_data(pick_data[i], (0, 0, -1*theta_z))\n","    aug_data.append(new_data_5)\n","    aug_data.append(new_data_6)\n","\n","  return aug_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"veFev-MbPWV5"},"source":["################################################################################\n","#                 AUGMENTING TRAINING AND VALIDATION DATASETS                  #\n","################################################################################\n","train_idle_aug = generate_aug_data(train_idle, degrees=(3,3,3), aug_ratio=1)\n","train_cruise_aug = generate_aug_data(train_cruise, degrees=(3,3,3), aug_ratio=1)\n","train_acc_aug = generate_aug_data(train_acc, degrees=(3,3,3), aug_ratio=1)\n","train_brake_aug = generate_aug_data(train_brake, degrees=(3,3,3), aug_ratio=1)\n","train_left_aug = generate_aug_data(train_left, degrees=(3,3,3), aug_ratio=1)\n","train_right_aug = generate_aug_data(train_right, degrees=(3,3,3), aug_ratio=1)\n","################################################################################\n","valid_idle_aug = generate_aug_data(valid_idle, degrees=(5,5,5), aug_ratio=0)\n","valid_cruise_aug = generate_aug_data(valid_cruise, degrees=(5,5,5), aug_ratio=0)\n","valid_acc_aug = generate_aug_data(valid_acc, degrees=(5,5,5), aug_ratio=0)\n","valid_brake_aug = generate_aug_data(valid_brake, degrees=(5,5,5), aug_ratio=0)\n","valid_left_aug = generate_aug_data(valid_left, degrees=(5,5,5), aug_ratio=0)\n","valid_right_aug = generate_aug_data(valid_right, degrees=(5,5,5), aug_ratio=0)\n","################################################################################\n","#                   SHOWING THE SIZES OF AUGMENTED DATASETS                    #\n","################################################################################\n","print('-----TRAINING-----')\n","print('Idle  :', data_size(train_idle_aug))\n","print('Cruise:', data_size(train_cruise_aug))\n","print('Acc   :', data_size(train_acc_aug))\n","print('Brake :', data_size(train_brake_aug))\n","print('Left  :', data_size(train_left_aug))\n","print('Right :', data_size(train_right_aug))\n","print('----VALIDATION-----')\n","print('Idle  :', data_size(valid_idle_aug))\n","print('Cruise:', data_size(valid_cruise_aug))\n","print('Acc   :', data_size(valid_acc_aug))\n","print('Brake :', data_size(valid_brake_aug))\n","print('Left  :', data_size(valid_left_aug))\n","print('Right :', data_size(valid_right_aug))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2EUDivGehJkc"},"source":["## f. Normalize dataset"]},{"cell_type":"markdown","metadata":{"id":"suBTxeBoxB_D"},"source":["### Calculating means and stds of training datasets"]},{"cell_type":"code","metadata":{"id":"v6Orj13GtXvB"},"source":["def cat_features(list_of_datasets):\n","  '''\n","  This function returns 6 lists which contain elements from 6 features,\n","  since the input list(list_of_datasets) have multiple datasets, \n","  the function must add elements from every dataset in the input list to the output lists\n","  '''\n","  ax ,ay, az, jx, jy, jz = list(), list(), list(), list(), list(), list()\n","  for i in range(len(list_of_datasets)):\n","    ax.extend(list(list_of_datasets[i][:,0]))\n","    ay.extend(list(list_of_datasets[i][:,1]))\n","    az.extend(list(list_of_datasets[i][:,2]))\n","    jx.extend(list(list_of_datasets[i][:,3]))\n","    jy.extend(list(list_of_datasets[i][:,4]))\n","    jz.extend(list(list_of_datasets[i][:,5]))\n","  return ax ,ay, az, jx, jy, jz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l2iFcRjls9f5"},"source":["def get_mean_and_std(datasets_0, datasets_1, datasets_2, datasets_3, datasets_4, datasets_5):\n","  '''\n","  The 6 input list of datasets are of different classes.\n","  Calling the \"cat_features()\" function to get output lists of each input list of datasets,\n","  then concatenate the 36 lists of features to 6 larger lists.\n","  Finally we can use np.mean and np.std to get means and stds.\n","  '''\n","  ax_0 ,ay_0, az_0, jx_0, jy_0, jz_0 = cat_features(datasets_0)\n","  ax_1 ,ay_1, az_1, jx_1, jy_1, jz_1 = cat_features(datasets_1)\n","  ax_2 ,ay_2, az_2, jx_2, jy_2, jz_2 = cat_features(datasets_2)\n","  ax_3 ,ay_3, az_3, jx_3, jy_3, jz_3 = cat_features(datasets_3)\n","  ax_4 ,ay_4, az_4, jx_4, jy_4, jz_4 = cat_features(datasets_4)\n","  ax_5 ,ay_5, az_5, jx_5, jy_5, jz_5 = cat_features(datasets_5)\n","\n","  ax = array(ax_0 + ax_1 + ax_2 + ax_3 + ax_4 + ax_5)\n","  ay = array(ay_0 + ay_1 + ay_2 + ay_3 + ay_4 + ay_5)\n","  az = array(az_0 + az_1 + az_2 + az_3 + az_4 + az_5)\n","  jx = array(jx_0 + jx_1 + jx_2 + jx_3 + jx_4 + jx_5)\n","  jy = array(jy_0 + jy_1 + jy_2 + jy_3 + jy_4 + jy_5)\n","  jz = array(jz_0 + jz_1 + jz_2 + jz_3 + jz_4 + jz_5)\n","\n","  means = [np.mean(ax), np.mean(ay), np.mean(az), np.mean(jx), np.mean(jy), np.mean(jz)]\n","  stds = [np.std(ax, ddof=0), np.std(ay, ddof=0), np.std(az, ddof=0), np.std(jx, ddof=0), np.std(jy, ddof=0), np.std(jz, ddof=0)]\n","\n","  return means, stds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jv1kKoi00mhm"},"source":["################################################################################\n","#               CALCULATING MEANS AND STDS OF \"TRAINING\" DATASETS              #\n","################################################################################\n","means, stds = get_mean_and_std(train_idle_aug, train_cruise_aug, train_acc_aug, train_brake_aug, train_left_aug, train_right_aug)\n","print('Means:', means)\n","print('Stds :', stds)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NznL1I0hxKIT"},"source":["### Nornalizing datasets"]},{"cell_type":"code","metadata":{"id":"wrB5lEJp7zS1"},"source":["def normalize(list_of_datasets, means, stds):\n","  '''\n","  N(x) = (x-mean)/std\n","  Using the above funtion to normalize the datasets in the input list\n","  '''\n","  norm_datasets = list_of_datasets\n","\n","  for i in range(len(list_of_datasets)):\n","    for j in range(list_of_datasets[i].shape[0]):\n","      seq = list_of_datasets[i][j] \n","      for k in range(6):\n","        seq[k] = (seq[k] - means[k])/stds[k]\n","      norm_datasets[i][j] = seq\n","  \n","  return norm_datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6xNl8phQpqJt"},"source":["################################################################################\n","#              NORMALIZING THE TRAINING AND VALIDATION DATASETS                #\n","################################################################################\n","train_idle_norm = normalize(train_idle_aug, means, stds)\n","train_cruise_norm = normalize(train_cruise_aug, means, stds)\n","train_acc_norm = normalize(train_acc_aug, means, stds)\n","train_brake_norm = normalize(train_brake_aug, means, stds)\n","train_left_norm = normalize(train_left_aug, means, stds)\n","train_right_norm = normalize(train_right_aug, means, stds)\n","################################################################################\n","valid_idle_norm = normalize(valid_idle_aug, means, stds)\n","valid_cruise_norm = normalize(valid_cruise_aug, means, stds)\n","valid_acc_norm = normalize(valid_acc_aug, means, stds)\n","valid_brake_norm = normalize(valid_brake_aug, means, stds)\n","valid_left_norm = normalize(valid_left_aug, means, stds)\n","valid_right_norm = normalize(valid_right_aug, means, stds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_aBEoAXMNH7g"},"source":["# demo\n","plot_data(train_right_norm[5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7qF7pBQLdWOz"},"source":["## g. Split data sequences "]},{"cell_type":"code","metadata":{"id":"UWNBplNPT4RT"},"source":["def split_dataset(dataset, n_steps):\n","  '''\n","  This function split input dataset into three outputs:\n","  x: an array with shape(len(dataset) - n_steps + 1, n_steps, 6), \n","  containing the information in a time interval has duration equals to n_steps.\n","  y1: the classification of motion of x.\n","  y2: the information next to the last time information of x.\n","  '''\n","  x, y1, y2 = list(), list(), list()\n","  # x: states of vehicle has duration is equal to n_steps \n","  # y1: category of the motion\n","  # y2: next state of the vehicle  \n","  for i in range(len(dataset)):\n","    end_ix = i + n_steps\n","    # check if index is output of bound\n","    if end_ix > len(dataset)-1:\n","      break\n","\n","    seq_x, seq_y1, seq_y2 = dataset[i:end_ix, :6], dataset[end_ix-1, 6:], dataset[end_ix, :3]\n","    x.append(seq_x)\n","    y1.append(seq_y1)\n","    y2.append(seq_y2)\n","  \n","  return array(x), array(y1), array(y2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lAHijnh9ctMN"},"source":["def generate_seq(list_of_datasets, n_steps):\n","  '''\n","  This is an higher level function,   which splits all datasets in \n","  list_of_datasets into sequences.\n","  '''\n","  x, y1, y2 = np.empty(shape=(0,n_steps,6)), np.empty(shape=(0,6)), np.empty(shape=(0,3))\n","  for i in range(len(list_of_datasets)):\n","    # Check if the dataset is longer than n_steps\n","    if list_of_datasets[i].shape[0] <= n_steps:\n","      #print('skip')\n","      continue\n","    tx, ty1, ty2 = split_dataset(list_of_datasets[i], n_steps)\n","    #print('i:',i,'/',tx.shape, ty1.shape, ty2.shape)\n","    x = np.append(x, tx, axis=0)\n","    y1 = np.append(y1, ty1, axis=0)\n","    y2 = np.append(y2, ty2, axis=0)\n","  \n","  return array(x), array(y1), array(y2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"khtGSho3q6QD"},"source":["################################################################################\n","#                 SPLITTING TRAINING AND VALIDATION SEQUENCES                  #\n","################################################################################\n","x_train_idle, y1_train_idle, y2_train_idle = generate_seq(train_idle_norm, n_steps)\n","x_train_cruise, y1_train_cruise, y2_train_cruise = generate_seq(train_cruise_norm, n_steps)\n","x_train_acc, y1_train_acc, y2_train_acc = generate_seq(train_acc_norm, n_steps)\n","x_train_brake, y1_train_brake, y2_train_brake = generate_seq(train_brake_norm, n_steps)\n","x_train_left, y1_train_left, y2_train_left = generate_seq(train_left_norm, n_steps)\n","x_train_right, y1_train_right, y2_train_right = generate_seq(train_right_norm, n_steps)\n","################################################################################\n","x_valid_idle, y1_valid_idle, y2_valid_idle = generate_seq(valid_idle_norm, n_steps)\n","x_valid_cruise, y1_valid_cruise, y2_valid_cruise = generate_seq(valid_cruise_norm, n_steps)\n","x_valid_acc, y1_valid_acc, y2_valid_acc = generate_seq(valid_acc_norm, n_steps)\n","x_valid_brake, y1_valid_brake, y2_valid_brake = generate_seq(valid_brake_norm, n_steps)\n","x_valid_left, y1_valid_left, y2_valid_left = generate_seq(valid_left_norm, n_steps)\n","x_valid_right, y1_valid_right, y2_valid_right = generate_seq(valid_right_norm, n_steps)\n","################################################################################\n","#                        SHOWING THE SIZE OF EACH CLASS                        #\n","################################################################################\n","print('----------TRAINING----------')\n","print('x_train_idle  :', x_train_idle.shape)\n","print('x_train_cruise:', x_train_cruise.shape)\n","print('x_train_acc   :', x_train_acc.shape)\n","print('x_train_brake :', x_train_brake.shape)\n","print('x_train_left  :', x_train_left.shape)\n","print('x_train_right :', x_train_right.shape)\n","print('---------VALIDATION---------')\n","print('x_valid_idle  :', x_valid_idle.shape)\n","print('x_valid_cruise:', x_valid_cruise.shape)\n","print('x_valid_acc   :', x_valid_acc.shape)\n","print('x_valid_brake :', x_valid_brake.shape)\n","print('x_valid_left  :', x_valid_left.shape)\n","print('x_valid_right :', x_valid_right.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MJYLGsdbTUdI"},"source":["## h. Concatenate and shuffle sequences"]},{"cell_type":"code","metadata":{"id":"ntn1MYFWTT3o"},"source":["################################################################################\n","#              CONCATENATING TRAINING AND VALIDATION SEQUENCES                 #\n","################################################################################\n","x_train = np.concatenate((x_train_idle, x_train_cruise, x_train_acc, x_train_brake, x_train_left, x_train_right))\n","y1_train = np.concatenate((y1_train_idle, y1_train_cruise, y1_train_acc, y1_train_brake, y1_train_left, y1_train_right))\n","y2_train = np.concatenate((y2_train_idle, y2_train_cruise, y2_train_acc, y2_train_brake, y2_train_left, y2_train_right))\n","################################################################################\n","x_valid = np.concatenate((x_valid_idle, x_valid_cruise, x_valid_acc, x_valid_brake, x_valid_left, x_valid_right))\n","y1_valid = np.concatenate((y1_valid_idle, y1_valid_cruise, y1_valid_acc, y1_valid_brake, y1_valid_left, y1_valid_right))\n","y2_valid = np.concatenate((y2_valid_idle, y2_valid_cruise, y2_valid_acc, y2_valid_brake, y2_valid_left, y2_valid_right))\n","################################################################################\n","#                 SHUFFLLING TRAINING AND VALIDATION SEQUENCES                 #\n","################################################################################\n","x_train, y1_train, y2_train = shuffle(x_train, y1_train, y2_train, random_state=0)\n","x_valid, y1_valid, y2_valid = shuffle(x_valid, y1_valid, y2_valid, random_state=0)\n","################################################################################\n","#               SHOWING THE SIZE OF TRAINING AND VALIDATION DATA               #\n","################################################################################\n","print('----TRAINING----')\n","print('x_train :', x_train.shape)\n","print('y1_train:', y1_train.shape)\n","print('y2_train:', y2_train.shape)\n","print('---VALIDATION---')\n","print('x_valid :', x_valid.shape)\n","print('y1_valid:', y1_valid.shape)\n","print('y2_valid:', y2_valid.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G5yx1bK0ebiH"},"source":["# Step 4: Build models"]},{"cell_type":"markdown","metadata":{"id":"ywZXj0LGCQXZ"},"source":["## 1.Classifier model"]},{"cell_type":"markdown","metadata":{"id":"7ZF1kb5u1cSD"},"source":["#### Inception module"]},{"cell_type":"code","metadata":{"id":"CTU1N-We3q3Q"},"source":["  def inception_cell_1D(inputs, num_bn, num_filters):\n","    # Step 1\n","    # Bottleneck and maxpooling layers\n","    bottleneck =layers.Conv1D(filters=num_bn, kernel_size=1, strides=1,\n","                         activation='relu', padding='same', use_bias=False)(inputs)\n","    maxpool = layers.MaxPool1D(pool_size=3,strides=1,padding='same')(inputs)\n","\n","    # Step 2\n","    # Convolution layers\n","    c0 = layers.Conv1D(filters=num_filters,kernel_size=20, strides=1,\n","                         activation='relu', padding='same', use_bias=False)(bottleneck)\n","    c1 = layers.Conv1D(filters=num_filters,kernel_size=10, strides=1,\n","                         activation='relu', padding='same', use_bias=False)(bottleneck)   \n","    c2 = layers.Conv1D(filters=num_filters,kernel_size=5, strides=1,\n","                         activation='relu', padding='same', use_bias=False)(bottleneck)      \n","    c3 = layers.Conv1D(filters=num_filters,kernel_size=1, strides=1,\n","                         activation='relu', padding='same', use_bias=False)(maxpool)\n","  \n","    # Step 3\n","    c = layers.concatenate([c0, c1, c2, c3], axis=2)     \n","    c = layers.BatchNormalization()(c)\n","    c = layers.Activation(activation='relu')(c) \n","    print('output:', c.shape)\n","    return c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Q5PZs_SGlPp"},"source":["### Short cut layer"]},{"cell_type":"code","metadata":{"id":"NdTpDRzZ3jc4"},"source":["def shortcut_layer(inputs, c_inception):\n","  # create shortcut connection\n","  c_shortcut = layers.Conv1D(filters=c_inception.shape[-1], kernel_size=1,\n","                             padding='same', use_bias=False)(inputs)\n","  c_shortcut = layers.BatchNormalization()(c_shortcut)\n","  # add short cut to inception\n","  c = layers.Add()([c_shortcut, c_inception])\n","\n","  return layers.Activation('relu')(c)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OxWsfCoC2iPr"},"source":["### Build model"]},{"cell_type":"code","metadata":{"id":"S78wY_jCMtqh"},"source":["input_layer_1D = keras.Input(shape=(n_steps,6), name='inputs')\n","c = input_layer_1D\n","\n","c_residual = input_layer_1D\n","\n","c = inception_cell_1D(c, 1, 4)\n","c = inception_cell_1D(c, 1, 4)\n","c = shortcut_layer(c_residual, c)\n","c_residual = c\n","c = inception_cell_1D(c, 1, 4)\n","'''\n","c = inception_cell_1D(c, 1, 4)\n","c = inception_cell_1D(c, 1, 4)\n","c = shortcut_layer(c_residual, c)\n","c_residual = c\n","c = inception_cell_1D(c, 1, 4)\n","'''\n","\n","d = layers.Flatten()(c)\n","#d = layers.Dropout(0.1)(d)\n","\n","output_layer = layers.Dense(n_class, activation='softmax')(d)\n","classifier = keras.Model(inputs=input_layer_1D, outputs=output_layer)\n","classifier.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pCfg_B14NQ1T"},"source":["keras.utils.plot_model(classifier, \"Classifier.png\", show_shapes=True, dpi=64)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hg3snQMPTYqU"},"source":["### Train the classifier model"]},{"cell_type":"code","metadata":{"id":"8bxnWf7RhwTe"},"source":["classifier_path = '/content/drive/My Drive/Colab Notebooks/KerasModels/Classifier/classifier.h5'\n","check_pt_c = keras.callbacks.ModelCheckpoint(classifier_path, monitor='val_loss', verbose=0, save_best_only=True)\n","classifier.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'],\n","              )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hcwU27sajayJ"},"source":["classifier_history = classifier.fit(\n","                    x_train, \n","                    y1_train,                    \n","                    epochs=200, \n","                    batch_size = 10000,\n","                    validation_data = (x_valid ,y1_valid),\n","                    verbose=1,\n","                    callbacks=check_pt_c\n","                    )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-ZMyVCDCTCgQ"},"source":["### Load the best model"]},{"cell_type":"code","metadata":{"id":"JK_eStRdpkbe"},"source":["classifier = tf.keras.models.load_model(classifier_path)\n","classifier.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-LZVihpSGZlY"},"source":["### Evaluation"]},{"cell_type":"code","metadata":{"id":"kTn1w6SyGRzM"},"source":["plt.plot(classifier_history.history['accuracy'])\n","plt.plot(classifier_history.history['val_accuracy'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n","\n","plt.plot(classifier_history.history['loss'])\n","plt.plot(classifier_history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dKXHNyUdG6Lw"},"source":["def confusion_matrix(pred, truth):\n","\n","  matrix = np.zeros(shape=(6, 6))\n","\n","  for i in range(pred.shape[0]):\n","    p = np.argmax(pred[i])\n","    t = np.argmax(truth[i])\n","    matrix[t][p] += 1;\n","\n","  return matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WFx34i7fGYKu"},"source":["np.set_printoptions(precision=3, suppress=True)\n","classifier_pred = classifier.predict(x_valid[:])\n","print(confusion_matrix(classifier_pred, y1_valid[:]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ASB-YetLMYVc"},"source":["## 2.Predictor model"]},{"cell_type":"markdown","metadata":{"id":"jtWi1X8pTN33"},"source":["### Build model"]},{"cell_type":"code","metadata":{"id":"OGG3-Nzzef_w"},"source":["input_layer = keras.Input(shape=(n_steps,6), name='inputs')\n","p = layers.Conv1D(filters=4,kernel_size=10,padding='same',activation='linear')(input_layer)\n","p = layers.Conv1D(filters=4,kernel_size=10,padding='same',activation='linear')(p)\n","p = layers.MaxPooling1D(pool_size=4)(p)\n","p = layers.Conv1D(filters=4,kernel_size=10,padding='same',activation='linear')(p)\n","p = layers.MaxPooling1D(pool_size=3)(p)\n","p = layers.Flatten()(p)\n","p = layers.Dense(64)(p)\n","p = layers.Dense(16)(p)\n","\n","outputs_2 = layers.Dense(3,activation='linear')(p)\n","predictor = keras.Model(inputs=input_layer,outputs=outputs_2,name='predictor_model')\n","\n","predictor.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fxkdEhQ8Nblx"},"source":["keras.utils.plot_model(predictor, \"Predictor.png\", show_shapes=True, dpi=64)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1i3lsJdeUPVQ"},"source":["### Train the predictor model"]},{"cell_type":"code","metadata":{"id":"pk8K7sZyUSqL"},"source":["predictor_path = '/content/drive/My Drive/Colab Notebooks/KerasModels/Predictor/predictor.h5'\n","check_pt_p = keras.callbacks.ModelCheckpoint(predictor_path, monitor='val_loss', verbose=0, save_best_only=True)\n","predictor.compile(optimizer='adam',\n","              loss=keras.losses.MeanSquaredError()\n","              )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dKb3bu4uUTSJ"},"source":["predictor_history = predictor.fit(\n","                    x_train, \n","                    y2_train,                    \n","                    epochs=400, \n","                    batch_size = 10000,\n","                    validation_data = (x_valid ,y2_valid),\n","                    verbose=1,\n","                    callbacks=check_pt_p\n","                    )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oqDHbR0WTXsI"},"source":["### Load the best model"]},{"cell_type":"code","metadata":{"id":"EN2j3idxp5Gq"},"source":["predictor = tf.keras.models.load_model(predictor_path)\n","predictor.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"55tg_RqkS0cV"},"source":["### Evaluation"]},{"cell_type":"code","metadata":{"id":"-swxTPK0Sj3r"},"source":["plt.plot(predictor_history.history['loss'])\n","plt.plot(predictor_history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w-VFSLPVSkdE"},"source":["test_data = read_file('/content/drive/My Drive/Colab Notebooks/Training/', '12.CSV', 6)\n","\n","list_data = [test_data]\n","normalize(list_data, means, stds)\n","test_seq , t1 , t2 = split_dataset(list_data[0], n_steps)\n","predictor_pred = predictor.predict(test_seq)\n","\n","plt.figure(figsize=(36,12))\n","\n","plt.subplot(311)\n","plt.plot(t2[:,0],label='Ground truth')\n","plt.plot(predictor_pred[1:,0],label='Prediction')\n","plt.title('ax')\n","plt.legend()\n","plt.subplot(312)\n","plt.plot(t2[:,1],label='Ground truth')\n","plt.plot(predictor_pred[1:,1],label='Prediction')  \n","plt.title('ay')\n","plt.legend()\n","plt.subplot(313)\n","plt.plot(t2[:,2],label='Ground truth')\n","plt.plot(predictor_pred[1:,2],label='Prediction')\n","plt.title('az')         \n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VgWUAfwHUOnj"},"source":["# Step 5: Store the models in .tf format"]},{"cell_type":"code","metadata":{"id":"F1LG8lAhVNr4"},"source":["classifier_name = 'classifier'\n","classifier_tf_path = 'KerasModels/Classifier/' + classifier_name + '.tf'\n","predictor_name = 'predictor'\n","predictor_tf_path = 'KerasModels/Predictor/' + predictor_name + '.tf'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_4PZXZS5XXyO"},"source":["## Classifier"]},{"cell_type":"code","metadata":{"id":"bDFZPmMtWuT8"},"source":["run_model_c = tf.function(lambda x: classifier(x))\n","# This is important, let's fix the input size.\n","BATCH_SIZE = 1\n","STEPS = n_steps\n","INPUT_SIZE = 6\n","concrete_func = run_model_c.get_concrete_function(\n","    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE, 1], classifier.inputs[0].dtype))\n","classifier.compile(optimizer='adam',\n","              loss='categorical_crossentropy',       ###############################\n","              metrics=['accuracy'],\n","              )\n","classifier.save(classifier_tf_path, signatures=concrete_func)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SwhZ0Z_qVne8"},"source":["load_classifier = tf.keras.models.load_model(classifier_tf_path)\n","load_classifier.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S-YZ9nvxV1LS"},"source":["classifier = load_classifier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QjMmHVbxXcOW"},"source":["## Predictor"]},{"cell_type":"code","metadata":{"id":"s-Qde13sXdsq"},"source":["run_model_p = tf.function(lambda x: predictor(x))\n","# This is important, let's fix the input size.\n","BATCH_SIZE = 1\n","STEPS = n_steps\n","INPUT_SIZE = 6\n","concrete_func = run_model_p.get_concrete_function(\n","    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE, 1], predictor.inputs[0].dtype))\n","predictor.compile(optimizer='adam',\n","              loss=keras.losses.MeanSquaredError(),\n","              )\n","predictor.save(predictor_tf_path, signatures=concrete_func)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Gb48sEcXd2i"},"source":["load_predictor = tf.keras.models.load_model(predictor_tf_path)\n","load_predictor.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R0uOXxbUXd9r"},"source":["predictor = load_predictor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n5p4G26-VAd6"},"source":["# Step 6: convert to tensorflow lite model"]},{"cell_type":"code","metadata":{"id":"ExeH5rxtV9yE"},"source":["TFLite_classifier_path = 'TFLiteModels/Classifier/' + classifier_name + '.tflite'\n","TFLite_predictor_path = 'TFLiteModels/Predictor/' + predictor_name + '.tflite'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXMD4123WEv4"},"source":["x_train_f32 = tf.cast(x_train, tf.float32)\n","def representative_data_gen():\n","  for input_value in tf.data.Dataset.from_tensor_slices(x_train_f32).batch(1).take(1000):\n","    yield [input_value]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OtjYGwDjxjys"},"source":["## Converting classifier model"]},{"cell_type":"code","metadata":{"id":"46ysE07bWH9d"},"source":["converter_c = tf.lite.TFLiteConverter.from_saved_model(classifier_tf_path)\n","converter_c.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter_c.representative_dataset = representative_data_gen\n","converter_c.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","converter_c.inference_input_type = tf.int8\n","converter_c.inference_output_type = tf.int8\n","TFLite_classifier = converter_c.convert()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a3A8wTcEXWS8"},"source":["open(TFLite_classifier_path, \"wb\").write(TFLite_classifier)\n","print(\"TFLite models and their sizes:\")\n","!ls \"TFLiteModels\" -lh"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ChaWEWNsx9V1"},"source":["## Converting predictor model"]},{"cell_type":"code","metadata":{"id":"4pg9DjnPyC7D"},"source":["converter_p = tf.lite.TFLiteConverter.from_saved_model(predictor_tf_path)\n","converter_p.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter_p.representative_dataset = representative_data_gen\n","converter_p.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","converter_p.inference_input_type = tf.int8\n","converter_p.inference_output_type = tf.int8\n","TFLite_predictor = converter_p.convert()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rRZ2t7m_nDRC"},"source":["open(TFLite_predictor_path, \"wb\").write(TFLite_predictor)\n","print(\"TFLite models and their sizes:\")\n","!ls \"TFLiteModels\" -lh"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cYpKsXNHxn6l"},"source":["## Convert predictor model"]},{"cell_type":"markdown","metadata":{"id":"EK_uQnS9dXJD"},"source":["# Step 7: Quantize tensorflow lite model"]},{"cell_type":"markdown","metadata":{"id":"HL1njL7WynkE"},"source":["## Classifier"]},{"cell_type":"code","metadata":{"id":"_jE2wqrOdd3f"},"source":["TFLite_interpreter_c = tf.lite.Interpreter(model_path=TFLite_classifier_path)\n","\n","input_details = TFLite_interpreter_c.get_input_details()\n","output_details = TFLite_interpreter_c.get_output_details()\n","\n","print(\"== Input details ==\")\n","print(\"name:\", input_details[0]['name'])\n","print(\"shape:\", input_details[0]['shape'])\n","print(\"type:\", input_details[0]['dtype'])\n","\n","print(\"\\n== Output details ==\")\n","print(\"name:\", output_details[0]['name'])\n","print(\"shape:\", output_details[0]['shape'])\n","print(\"type:\", output_details[0]['dtype'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wvDPf_eldhDk"},"source":["TFLite_interpreter_c.resize_tensor_input(input_details[0]['index'], (1, n_steps, 6))\n","TFLite_interpreter_c.resize_tensor_input(output_details[0]['index'], (1,n_class))\n","TFLite_interpreter_c.allocate_tensors()\n","\n","input_details = TFLite_interpreter_c.get_input_details()\n","output_details = TFLite_interpreter_c.get_output_details()\n","\n","print(\"== Input details ==\")\n","print(\"name:\", input_details[0]['name'])\n","print(\"shape:\", input_details[0]['shape'])\n","print(\"type:\", input_details[0]['dtype'])\n","\n","print(\"\\n== Output details ==\")\n","print(\"name:\", output_details[0]['name'])\n","print(\"shape:\", output_details[0]['shape'])\n","print(\"type:\", output_details[0]['dtype'])\n","input_scale_c = input_details[0]['quantization'][0]\n","input_zero_point_c = input_details[0]['quantization_parameters']['zero_points'][0]\n","print(input_scale_c, input_zero_point_c)\n","output_scale_c = output_details[0]['quantization'][0]\n","output_zero_point_c = float(output_details[0]['quantization_parameters']['zero_points'][0])\n","print(output_scale_c, output_zero_point_c)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-s7ncfTrdwHR"},"source":["TFLite_classifications = []\n","for n in range(x_valid.shape[0]):\n","  test_data = x_valid[n:n+1]\n","  TFLite_interpreter_c.set_tensor(input_details[0]['index'], \n","                                ((((test_data)/input_scale_c)+input_zero_point_c).astype('int8')))\n","  TFLite_interpreter_c.invoke()\n","  new_classifications = TFLite_interpreter_c.get_tensor(output_details[0]['index'])\n","\n","\n","  TFLite_classifications.append((new_classifications-output_zero_point_c)*output_scale_c)\n","keras_classifications = classifier.predict(x_valid[:])\n","\n","count = 0\n","\n","for i in range(x_valid.shape[0]):\n","  if np.argmax(keras_classifications[i]) == np.argmax(TFLite_classifications[i]):\n","    count += 1\n","\n","print(count/x_valid.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O9QvIIqky5Cr"},"source":["## Predictor"]},{"cell_type":"code","metadata":{"id":"v1nb0Gt2y8jx"},"source":["TFLite_interpreter_p = tf.lite.Interpreter(model_path=TFLite_predictor_path)\n","\n","input_details = TFLite_interpreter_p.get_input_details()\n","output_details = TFLite_interpreter_p.get_output_details()\n","\n","print(\"== Input details ==\")\n","print(\"name:\", input_details[0]['name'])\n","print(\"shape:\", input_details[0]['shape'])\n","print(\"type:\", input_details[0]['dtype'])\n","\n","print(\"\\n== Output details ==\")\n","print(\"name:\", output_details[0]['name'])\n","print(\"shape:\", output_details[0]['shape'])\n","print(\"type:\", output_details[0]['dtype'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zeqmZI4FzG0L"},"source":["TFLite_interpreter_p.resize_tensor_input(input_details[0]['index'], (1, n_steps, 6))\n","TFLite_interpreter_p.resize_tensor_input(output_details[0]['index'], (1,3))\n","TFLite_interpreter_p.allocate_tensors()\n","\n","input_details = TFLite_interpreter_p.get_input_details()\n","output_details = TFLite_interpreter_p.get_output_details()\n","\n","print(\"== Input details ==\")\n","print(\"name:\", input_details[0]['name'])\n","print(\"shape:\", input_details[0]['shape'])\n","print(\"type:\", input_details[0]['dtype'])\n","\n","print(\"\\n== Output details ==\")\n","print(\"name:\", output_details[0]['name'])\n","print(\"shape:\", output_details[0]['shape'])\n","print(\"type:\", output_details[0]['dtype'])\n","input_scale_p = input_details[0]['quantization'][0]\n","input_zero_point_p = input_details[0]['quantization_parameters']['zero_points'][0]\n","print(input_scale_p, input_zero_point_p)\n","output_scale_p = output_details[0]['quantization'][0]\n","output_zero_point_p = float(output_details[0]['quantization_parameters']['zero_points'][0])\n","print(output_scale_p, output_zero_point_p)"],"execution_count":null,"outputs":[]}]}